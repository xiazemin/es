**架构**

**节点类型**

ES的架构很简单，集群的HA不需要依赖任务外部组件（例如Zookeeper、HDFS等），master节点的主备依赖于内部自建的选举算法，通过副本分片的方式实现了数据的备份的同时，也提高了并发查询的能力。

ES集群的服务器分为以下四种角色：

* master节点，负责保存和更新集群的一些元数据信息，之后同步到所有节点，所以每个节点都需要保存全量的元数据信息：

  ▫集群的配置信息

  ▫集群的节点信息

  ▫模板template设置

  ▫索引以及对应的设置、mapping、分词器和别名

  ▫索引关联到的分片以及分配到的节点

* datanode：负责数据存储和查询
* coordinator：

  ▫路由索引请求

  ▫聚合搜索结果集

  ▫分发批量索引请求

* ingestor：

  ▫类似于logstash，对输入数据进行处理和转换

如何配置节点类型

* 一个节点的缺省配置是：主节点+数据节点两属性为一身。对于3-5个节点的小集群来讲，通常让所有节点存储数据和具有获得主节点的资格。
* 专用协调节点（也称为client节点或路由节点）从数据节点中消除了聚合/查询的请求解析和最终阶段，随着集群写入以及查询负载的增大，可以通过协调节点减轻数据节点的压力，可以让数据节点更多专注于数据的写入以及查询。

**master选举**

选举策略

* 如果集群中存在master，认可该master，加入集群
* 如果集群中不存在master，从具有master资格的节点中选id最小的节点作为master

选举时机

集群启动：后台启动线程去ping集群中的节点，按照上述策略从具有master资格的节点中选举出master

现有的master离开集群：后台一直有一个线程定时ping master节点，超过一定次数没有ping成功之后，重新进行master的选举

选举流程

![](https://pic2.zhimg.com/80/v2-8901bac93e818120cd751597282c3965_hd.jpg)

避免脑裂

脑裂问题是采用master-slave模式的分布式集群普遍需要关注的问题，脑裂一旦出现，会导致集群的状态出现不一致，导致数据错误甚至丢失。

ES避免脑裂的策略：过半原则，可以在ES的集群配置中添加一下配置，避免脑裂的发生

\#一个节点多久ping一次，默认1s

discovery.zen.fd.ping\_interval: 1s

\#\#等待ping返回时间，默认30s

discovery.zen.fd.ping\_timeout: 10s

\#\#ping超时重试次数，默认3次

discovery.zen.fd.ping\_retries: 3

\#\#选举时需要的节点连接数，N为具有master资格的节点数量

discovery.zen.minimum\_master\_nodes=N/2+1



